# 1. Identify a hot topic in climate change
#   a. carbon emission; climate crisis
#   b. mentioned in presentation

# 2. Use a Newspaper API to retrieve 20 search outcomes
# - retrieve 20 search results using an API request
# - parse <body> data from JSON response
# - store each of the results in /documents/news-articles/ as .txt files
# - retrieve the html of each page and "scrape" the comment section text
# - store (at least) 5 comments of the results in /documents/comments/ as .txt files
# - Total generated docs = 20 + (20 * 5) = 120 docs

# 3. Check the overlapping extent (similarities) between news_article_doc and comments_doc
# - preprocessing => tokenization
# - preprocessing => stopword removal
# - preprocessing => stemming
# - generate a histogram of the most frequent words (excluding stopwords) of each news_article_doc & their comments_doc
# - calculate the Jaccard index between news_article_doc and comments_doc

# 4. Identify the topics of the news_article_doc and its comments_doc using the LDA model
#  - LDA model: 3 topics, 5 words per topic
# TODO: create a list of words generated by LDA for news_article_doc [L1]
# TODO: create a list of words generated by LDA for comments_doc [L2]
# TODO: calculate the Jaccard index between L1 and L2

# 5. Perform Sentiment Analysis on each news_article_doc and its comments_doc
# TODO: use SentiStrength to calculate the vector positive & negative sentiment for each news_article_doc [senti_news_doc]
# TODO: use SentiStrength to calculate the vector positive & negative sentiment for each comments_doc [senti_comments_doc]
# TODO: calculate the Pearson correlation between each senti_news_doc & their senti_comments_doc

### <-- COMPLETE BY SUNDAY, 06/11/2022 18:59 --> ###

# 6. Evaluate the agreement/disagreement extent for each comments_doc
# TODO: get a list of Negative Emotion Wordings from any corpus (e.g. Empath)
# TODO: identify the entity that the Negative Sentiment Word (if any) is associated to, using a Parser Tree
# TODO: generate a histogram of these entities (across all the comments_docs of a news_article_doc)

# 7. Investigate the behaviour of user comments (across all the comments_docs of a news_article_doc)
# TODO: create a list of agreement words [list_agree]
# TODO: create a list of disagreement words [list_disagree]
# TODO: count the occurrences of all agreement-related words in all the comments_docs of a news_article_doc [cnt_agr]
# TODO: count the occurrences of all disagreement-related words in all the comments_docs of a news_article_doc [cnt_dis]
# TODO: generate a histogram of the cnt_agr & cnt_dis

### <-- COMPLETE BY FRIDAY, 04/11/2022 18:00 --> ###

# 8. Create a GUI to demonstrate tasks 3-7 (maybe 2)
# TODO: design GUI with Figma
# TODO: export Figma GUI with Tkinter*

### <-- COMPLETE ON FRIDAY, 04/11/2022 --> ###

import nltk
import string
import requests
import json
import matplotlib.pyplot as matplot
import pandas
import numpy as np
import seaborn as sns
from nltk import word_tokenize
from urllib import request
from newsapi import NewsApiClient
from bs4 import BeautifulSoup
from nltk.stem import WordNetLemmatizer
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
from scipy.spatial.distance import cosine, jaccard
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

# 2.1. retrieve 20 search results using an API request
def retrieve_search_results(keyword: string, search_cnt: int = 20):
    # Using The Guardian API
    url = 'https://content.guardianapis.com/search'
    params = {
        'api-key': 'test',
        'q': keyword,
        'page-size': search_cnt,
        'section': 'commentisfree',
        'format': 'json',
        'from-date': '2022-08-01',
        'show-fields': 'headline,short-url,body',
        'order-by': 'relevance'
    }
    api_result = requests.get(url, params)
    json_str = json.dumps(api_result.json())
    return json.loads(json_str)["response"]["results"]

# 2.2. parse <body> data from JSON response
# 2.3. store each of the results in /documents/news-articles/ as .txt files
def parse_news_articles(news_articles: string):
    count_articles = 0
    articles_w_comments = []
    # parse the search results to extract the text in their <body>
    for i in range(0, len(news_articles), 1):
        body = news_articles[i]["fields"]["body"]
        raw_news_text = BeautifulSoup(body, 'html.parser').get_text()
        # Check if article has comments
        root_url = "http://discussion.theguardian.com/discussion-api/discussion"
        comm = news_articles[i]["fields"]["shortUrl"].replace("https://www.theguardian.com", "/")
        comment_url = root_url + comm + "/topcomments?orderBy=newest&page=1&pageSize=5"
        comm_results = requests.get(comment_url)
        if count_articles <= 20:
            # Save articles that contain comments
            if comm_results.json()["status"] == 'ok':
                file_name = 'news_' + str(count_articles) + '.txt'
                with open("documents/news-articles/" + file_name, "w+") as output_file:
                    output_file.write(raw_news_text)
                count_articles = count_articles + 1
                articles_w_comments.append(news_articles[i])
        else:
            break
    return articles_w_comments

def get_comments_from_articles(news_articles: string):
    ## API request for comments:
    # - http://discussion.theguardian.com/discussion-api/discussion//p/mek3c?orderBy=oldest&page=1&pageSize=5
    root_url = "http://discussion.theguardian.com/discussion-api/discussion"
    for i in range(0, len(news_articles), 1):
        comm = news_articles[i]["fields"]["shortUrl"].replace("https://www.theguardian.com", "/")
        comment_url = root_url + comm + "/topcomments?orderBy=newest&page=1&pageSize=5"
        comm_results = requests.get(comment_url)
        json_str = json.dumps(comm_results.json())
        comments = json.loads(json_str)["discussion"]["comments"]
        for j in range(0, len(comments), 1):
            comment = comments[j]["body"]
            raw_comment_text = BeautifulSoup(comment, 'html.parser').get_text()
            file_name = 'news_' + str(i) + '_comment_' + str(j) + '.txt'
            with open("documents/comments/" + file_name, "w+") as output_file:
                output_file.write(raw_comment_text)

def preprocess_doc(doc_directory: string, doc_name: string):
    # get text from file
    try:
        with open(doc_directory + doc_name + ".txt", "r") as input_file:
            file_data = input_file.read()
        # tokenization
        word_tokens = word_tokenize(file_data)
        # stopword removal
        stopwords = list(set(nltk.corpus.stopwords.words('english')))
        words_sw = [word for word in word_tokens if word.isalpha() and word not in stopwords]
        # stemming
        stemmer = SnowballStemmer("english")
        words_st = [stemmer.stem(word) for word in words_sw]
        parsed_doc = words_st
        print(parsed_doc)
        return parsed_doc
    except (FileNotFoundError):
        return None

# generate a histogram of the most frequent words (excluding stopwords) of each news_article_doc & their comments_doc
def get_term_frequency_dist(term_tokens, nr_of_terms: int):
    tfd = nltk.FreqDist(term_tokens)
    mct = tfd.most_common(nr_of_terms)
    return pandas.Series(dict(mct))

def plot_histogram(tfs, title, xlabel, ylabel):
    fig, axes = matplot.subplots(figsize=(10, 10))
    hist_plot = sns.barplot(x=tfs.index, y=tfs.values, ax=axes, palette='crest')
    matplot.xticks(rotation=45)
    matplot.title(title)
    matplot.ylabel(ylabel)
    matplot.xlabel(xlabel)
    matplot.show()
def view_most_frequent_terms(start_index: int, end_index: int):
    for i in range(start_index, end_index, 1):
        news_doc_name = "news_" + str(i)
        news_tkns = preprocess_doc("documents/news-articles/", news_doc_name)
        if (news_tkns != None):
            # plot histogram for article
            tfs_art = get_term_frequency_dist(news_tkns, 20)
            plot_histogram(tfs_art, 'Term frequency of article ' + news_doc_name, 'Terms', 'Frequency')
            # plot histograms for comments
            for j in range(0, 4, 1):
                comm_doc_name = "news_" + str(i) + "_comment_" + str(j)
                comm_tkns = preprocess_doc("documents/comments/", comm_doc_name)
                if (comm_tkns != None):
                    tfs_comm = get_term_frequency_dist(comm_tkns, 20)
                    plot_histogram(tfs_comm, 'Term frequency of comment ' + comm_doc_name, 'Terms', 'Frequency')
                    # Jaccard Index between news_article_doc and comments_doc
                    print("Jaccard Index of " + news_doc_name + " & " + comm_doc_name + " : " + str(
                        jaccard_similarity(tfs_art.keys(), tfs_comm.keys())))
                else:
                    continue
        else:
            continue

def jaccard_similarity(A, B):
    return len(A.intersection(B)) / len(A.union(B))


# TODO: create a list of words generated by LDA for news_article_doc [L1]
# TODO: create a list of words generated by LDA for comments_doc [L2]
# TODO: calculate the Jaccard index between L1 and L2

## Project tasks ##
results = retrieve_search_results('carbon emissions', 40)
news_articles_w_comments = parse_news_articles(results)
get_comments_from_articles(news_articles_w_comments)
view_most_frequent_terms(1, 20)
