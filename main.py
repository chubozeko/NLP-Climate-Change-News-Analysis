# 1. Identify a hot topic in climate change
#   a. carbon emission; climate crisis
#   b. mentioned in presentation

# 2. Use a Newspaper API to retrieve 20 search outcomes
# - retrieve 20 search results using an API request
# - parse <body> data from JSON response
# - store each of the results in /documents/news-articles/ as .txt files
# - retrieve the html of each page and "scrape" the comment section text
# - store (at least) 5 comments of the results in /documents/comments/ as .txt files
# - Total generated docs = 20 + (20 * 5) = 120 docs

# 3. Check the overlapping extent (similarities) between news_article_doc and comments_doc
# - preprocessing => tokenization
# - preprocessing => stopword removal
# - preprocessing => stemming
# - generate a histogram of the most frequent words (excluding stopwords) of each news_article_doc & their comments_doc
# - calculate the Jaccard index between news_article_doc and comments_doc

# 4. Identify the topics of the news_article_doc and its comments_doc using the LDA model
#  - LDA model: 3 topics, 5 words per topic
# TODO: create a list of words generated by LDA for news_article_doc [L1]
# TODO: create a list of words generated by LDA for comments_doc [L2]
# TODO: calculate the Jaccard index between L1 and L2

# 5. Perform Sentiment Analysis on each news_article_doc and its comments_doc
# - use SentiStrength to calculate the vector positive & negative sentiment for each news_article_doc [senti_news_doc]
# - use SentiStrength to calculate the vector positive & negative sentiment for each comments_doc [senti_comments_doc]
# - calculate the Pearson correlation between each senti_news_doc & their senti_comments_doc

# 6. Evaluate the agreement/disagreement extent for each comments_doc
# - get a list of Negative Emotion Wordings from any corpus (e.g. Empath)
# - identify the entity that the Negative Sentiment Word (if any) is associated to, using a Parser Tree
# - generate a histogram of these entities (across all the comments_docs of a news_article_doc)

# 7. Investigate the behaviour of user comments (across all the comments_docs of a news_article_doc)
# - create a list of agreement words [list_agree]
# - create a list of disagreement words [list_disagree]
# - count the occurrences of all agreement-related words in all the comments_docs of a news_article_doc [cnt_agr]
# - count the occurrences of all disagreement-related words in all the comments_docs of a news_article_doc [cnt_dis]
# - generate a histogram of the cnt_agr & cnt_dis

# 8. Create a GUI to demonstrate tasks 3-7 (maybe 2)
# TODO: design GUI with Figma
# TODO: export Figma GUI with Tkinter*

import nltk
import string
import requests
import json
import matplotlib.pyplot as matplot
import pandas
import numpy as np
import seaborn as sns
from nltk import word_tokenize
from urllib import request
from newsapi import NewsApiClient
from bs4 import BeautifulSoup
from nltk.stem import WordNetLemmatizer
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
from scipy.spatial.distance import cosine, jaccard
from scipy.stats import pearsonr
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
import gensim
from gensim import corpora
import pyLDAvis
# from sentistrength import PySentiStr
# import pyLDAvis.gensim
from textblob import TextBlob
from textblob.sentiments import NaiveBayesAnalyzer
from empath import Empath
from nltk.corpus import wordnet as wn
from nltk.corpus import sentiwordnet as swn

# 2.1. retrieve 20 search results using an API request
def retrieve_search_results(keyword: string, search_cnt: int = 20):
    # Using The Guardian API
    url = 'https://content.guardianapis.com/search'
    params = {
        'api-key': 'test',
        'q': keyword,
        'page-size': search_cnt,
        'section': 'commentisfree',
        'format': 'json',
        'from-date': '2022-08-01',
        'show-fields': 'headline,short-url,body',
        'order-by': 'relevance'
    }
    api_result = requests.get(url, params)
    json_str = json.dumps(api_result.json())
    return json.loads(json_str)["response"]["results"]

# 2.2. parse <body> data from JSON response
# 2.3. store each of the results in /documents/news-articles/ as .txt files
def parse_news_articles(news_articles: string):
    count_articles = 0
    articles_w_comments = []
    # parse the search results to extract the text in their <body>
    for i in range(0, len(news_articles), 1):
        body = news_articles[i]["fields"]["body"]
        raw_news_text = BeautifulSoup(body, 'html.parser').get_text()
        # Check if article has comments
        root_url = "http://discussion.theguardian.com/discussion-api/discussion"
        comm = news_articles[i]["fields"]["shortUrl"].replace("https://www.theguardian.com", "/")
        comment_url = root_url + comm + "/topcomments?orderBy=newest&page=1&pageSize=5"
        comm_results = requests.get(comment_url)
        if count_articles <= 20:
            # Save articles that contain comments
            if comm_results.json()["status"] == 'ok':
                file_name = 'news_' + str(count_articles) + '.txt'
                with open("documents/news-articles/" + file_name, "w+") as output_file:
                    output_file.write(raw_news_text)
                count_articles = count_articles + 1
                articles_w_comments.append(news_articles[i])
        else:
            break
    return articles_w_comments

def get_comments_from_articles(news_articles: string):
    ## API request for comments:
    # - http://discussion.theguardian.com/discussion-api/discussion//p/mek3c?orderBy=oldest&page=1&pageSize=5
    root_url = "http://discussion.theguardian.com/discussion-api/discussion"
    for i in range(0, len(news_articles), 1):
        comm = news_articles[i]["fields"]["shortUrl"].replace("https://www.theguardian.com", "/")
        comment_url = root_url + comm + "/topcomments?orderBy=newest&page=1&pageSize=5"
        comm_results = requests.get(comment_url)
        json_str = json.dumps(comm_results.json())
        comments = json.loads(json_str)["discussion"]["comments"]
        for j in range(0, len(comments), 1):
            comment = comments[j]["body"]
            raw_comment_text = BeautifulSoup(comment, 'html.parser').get_text()
            file_name = 'news_' + str(i) + '_comment_' + str(j) + '.txt'
            with open("documents/comments/" + file_name, "w+") as output_file:
                output_file.write(raw_comment_text)

def preprocess_doc(doc_directory: string, doc_name: string):
    # get text from file
    try:
        with open(doc_directory + doc_name + ".txt", "r") as input_file:
            file_data = input_file.read()
        # tokenization
        word_tokens = word_tokenize(file_data)
        # stopword removal
        stopwords = list(set(nltk.corpus.stopwords.words('english')))
        words_sw = [word for word in word_tokens if word.isalpha() and word not in stopwords]
        # stemming
        stemmer = SnowballStemmer("english")
        words_st = [stemmer.stem(word) for word in words_sw]
        # lemmatizing
        WNLemmatizer = WordNetLemmatizer()
        words_le = [WNLemmatizer.lemmatize(word, pos="v") for word in words_st]
        parsed_doc = words_le
        # print(parsed_doc)
        return parsed_doc
    except (FileNotFoundError):
        return None

def load_doc_file(doc_directory: string, doc_name: string):
    # get text from file
    try:
        with open(doc_directory + doc_name + ".txt", "r") as input_file:
            file_data = input_file.read()
        return file_data
    except (FileNotFoundError):
        return None

# generate a histogram of the most frequent words (excluding stopwords) of each news_article_doc & their comments_doc
def get_term_frequency_dist(term_tokens, nr_of_terms: int):
    tfd = nltk.FreqDist(term_tokens)
    mct = tfd.most_common(nr_of_terms)
    return pandas.Series(dict(mct))

def plot_histogram(tfs, title, xlabel, ylabel):
    fig, axes = matplot.subplots(figsize=(10, 10))
    hist_plot = sns.barplot(x=tfs.index, y=tfs.values, ax=axes, palette='crest')
    matplot.xticks(rotation=45)
    matplot.title(title)
    matplot.ylabel(ylabel)
    matplot.xlabel(xlabel)
    matplot.show()
def view_most_frequent_terms(start_index: int, end_index: int):
    for i in range(start_index, end_index, 1):
        news_doc_name = "news_" + str(i)
        news_tkns = preprocess_doc("documents/news-articles/", news_doc_name)
        if (news_tkns != None):
            # plot histogram for article
            tfs_art = get_term_frequency_dist(news_tkns, 20)
            plot_histogram(tfs_art, 'Term frequency of article ' + news_doc_name, 'Terms', 'Frequency')
            # plot histograms for comments
            for j in range(0, 4, 1):
                comm_doc_name = "news_" + str(i) + "_comment_" + str(j)
                comm_tkns = preprocess_doc("documents/comments/", comm_doc_name)
                if (comm_tkns != None):
                    tfs_comm = get_term_frequency_dist(comm_tkns, 20)
                    plot_histogram(tfs_comm, 'Term frequency of comment ' + comm_doc_name, 'Terms', 'Frequency')
                    # Jaccard Index between news_article_doc and comments_doc
                    print("Jaccard Index of " + news_doc_name + " & " + comm_doc_name + " : " + str(
                        jaccard_similarity(tfs_art.keys(), tfs_comm.keys())))
                else:
                    continue
        else:
            continue

def jaccard_similarity(A, B):
    return len(A.intersection(B)) / len(A.union(B))

def identify_topics_with_lda(start_index: int, end_index: int):
    for i in range(start_index, end_index, 1):
        news_doc_name = "news_" + str(i)
        news_tkns = preprocess_doc("documents/news-articles/", news_doc_name)
        if (news_tkns != None):
            tfs_art = get_term_frequency_dist(news_tkns, 20)
            # run_lda_model(news_tkns)
            for j in range(0, 4, 1):
                comm_doc_name = "news_" + str(i) + "_comment_" + str(j)
                comm_tkns = preprocess_doc("documents/comments/", comm_doc_name)
                if (comm_tkns != None):
                    tfs_comm = get_term_frequency_dist(comm_tkns, 20)
                    # run_lda_model(comm_tkns)
                else:
                    continue
        else:
            continue
def run_lda_model(doc_tokens):
    word_dictionary = corpora.Dictionary(doc_tokens)
    print(word_dictionary)
    corpus = [word_dictionary.doc2bow(t) for t in doc_tokens]
    lda_model = gensim.models.ldamodel.LdaModel(corpus=doc_tokens,
                                                id2word=word_dictionary,
                                                num_topics=3,
                                                chunksize=100,
                                                eta='5.0 / num_topics',
                                                per_word_topics=True)
    print(lda_model.print_topics())

def get_sentiment_of_doc(doc: string):
    senti_class = ''
    blob = TextBlob(doc, analyzer=NaiveBayesAnalyzer())
    senti_vec = np.array([blob.sentiment.p_pos, blob.sentiment.p_neg])
    if (str(blob.sentiment.classification) == 'pos'):
        senti_class = 'Positive'
    elif (str(blob.sentiment.classification) == 'neg'):
        senti_class = 'Negative'
    else:
        senti_class = 'Neutral'
    return senti_vec, senti_class

def pearson_correlation(x, y):
    return pearsonr(x, y).statistic

def view_sentiments(start_doc_index: int, end_doc_index: int):
    for i in range(start_doc_index, end_doc_index, 1):
        news_doc_name = "news_" + str(i)
        news_text = load_doc_file("documents/news-articles/", news_doc_name)
        if (news_text != None):
            # get sentiment vector of article
            news_senti_vec, news_senti_class = get_sentiment_of_doc(news_text)
            print('Sentiments for ' + news_doc_name + ' [positive, negative]: ' + str(news_senti_vec))
            print('-> Overall sentiment classification: ' + news_senti_class)
            for j in range(0, 4, 1):
                comm_doc_name = "news_" + str(i) + "_comment_" + str(j)
                comm_text = load_doc_file("documents/comments/", comm_doc_name)
                if (comm_text != None):
                    # get sentiment vector of comment
                    comm_senti_vec, comm_senti_class = get_sentiment_of_doc(comm_text)
                    print('Sentiments for ' + comm_doc_name + ' [positive, negative]: ' + str(comm_senti_vec))
                    print('-> Overall sentiment classification: ' + comm_senti_class)
                    # Pearson correlation between news_article & comment
                    print('-> Pearson correlation between ' + news_doc_name + ' & ' + comm_doc_name + ' = ' +
                          str(pearson_correlation(news_senti_vec, comm_senti_vec)))
                else:
                    continue
        else:
            continue
        print('---------------------------------------------------------------------------')


def find_negative_emotions(doc, doc_name):
    negative_words = []
    lexicon = Empath()
    # identify the sentence that the Negative Emotion is associated with
    sent_tokens = sent_tokenize(doc)
    for i in range(0, len(sent_tokens), 1):
        # get a list of Negative Emotion wordings from Empath
        sent_emotion = lexicon.analyze(sent_tokens[i], categories=["negative_emotion"], normalize=True)
        if (sent_emotion["negative_emotion"] != 0.0):
            # identify the entities that the Negative Emotion is associated with
            word_tokens = word_tokenize(sent_tokens[i])
            for j in range(0, len(word_tokens), 1):
                # get a list of Negative Emotion wordings from Empath
                word_emotion = lexicon.analyze(word_tokens[j], categories=["negative_emotion"], normalize=True)
                if (word_emotion["negative_emotion"] != 0.0):
                    negative_words.append((word_tokens[j], sent_emotion["negative_emotion"],))
    if (len(negative_words) != 0):
        # draw a histogram of these negative entities and their influence on their sentences
        plot_histogram(pandas.Series(dict(negative_words)), 'Negative Emotion Entites from ' + doc_name, 'Entities', 'Negative sentiment value on sentence')

def view_user_disagreement_extent(start_doc_index: int, end_doc_index: int):
    for i in range(start_doc_index, end_doc_index, 1):
        for j in range(0, 4, 1):
            comm_doc_name = "news_" + str(i) + "_comment_" + str(j)
            comm_text = load_doc_file("documents/comments/", comm_doc_name)
            if (comm_text != None):
                # find the negative emotion entities from comment doc (if any)
                find_negative_emotions(comm_text, comm_doc_name)
            else:
                continue
        else:
            continue
        print('---------------------------------------------------------------------------')

def find_positive_emotions(doc, doc_name):
    positive_words = []
    lexicon = Empath()
    # identify the sentence that the Positive Emotion is associated with
    sent_tokens = sent_tokenize(doc)
    for i in range(0, len(sent_tokens), 1):
        # get a list of Positive Emotion wordings from Empath
        sent_emotion = lexicon.analyze(sent_tokens[i], categories=["positive_emotion"], normalize=True)
        if (sent_emotion["positive_emotion"] != 0.0):
            # identify the entities that the Positive Emotion is associated with
            word_tokens = word_tokenize(sent_tokens[i])
            for j in range(0, len(word_tokens), 1):
                # get a list of Positive Emotion wordings from Empath
                word_emotion = lexicon.analyze(word_tokens[j], categories=["positive_emotion"], normalize=True)
                if (word_emotion["positive_emotion"] != 0.0):
                    positive_words.append((word_tokens[j], sent_emotion["positive_emotion"],))
    if (len(positive_words) != 0):
        # draw a histogram of these negative entities and their influence on their sentences
        plot_histogram(pandas.Series(dict(positive_words)), 'Positive Emotion Entites from ' + doc_name, 'Entities', 'Positive sentiment value on sentence')

def view_user_agreement_extent(start_doc_index: int, end_doc_index: int):
    for i in range(start_doc_index, end_doc_index, 1):
        for j in range(0, 4, 1):
            comm_doc_name = "news_" + str(i) + "_comment_" + str(j)
            comm_text = load_doc_file("documents/comments/", comm_doc_name)
            if (comm_text != None):
                # find the positive emotion entities from comment doc (if any)
                find_positive_emotions(comm_text, comm_doc_name)
            else:
                continue
        else:
            continue
        print('---------------------------------------------------------------------------')

# - create a list of agreement words [list_agree]
def get_agreement_words():
    list_agree = []
    agreement_seeds = ['agree', 'agreement', 'yes', 'sure', 'right']
    for i in range(0, len(agreement_seeds), 1):
        for synset in wn.synsets(agreement_seeds[i]):
            for lem in synset.lemmas():
                if lem.name() not in list_agree:
                    list_agree.append(lem.name())
    return list_agree

# - create a list of disagreement words [list_disagree]
def get_disagreement_words():
    list_disagree = []
    disagreement_seeds = ['disagree', 'disagreement', 'no', 'unsure', 'wrong']
    for i in range(0, len(disagreement_seeds), 1):
        for synset in wn.synsets(disagreement_seeds[i]):
            for lem in synset.lemmas():
                if lem.name() not in list_disagree:
                    list_disagree.append(lem.name())
    return list_disagree

# - count the occurrences of all agreement-related words in all the comments_docs of a news_article_doc [cnt_agr]
def get_agreement_word_count(doc):
    cnt_agr = 0
    tokens = word_tokenize(doc)
    list_agree = get_agreement_words()
    for i in range(0, len(tokens), 1):
        if tokens[i] in list_agree:
            cnt_agr = cnt_agr + 1
    return cnt_agr

# - count the occurrences of all disagreement-related words in all the comments_docs of a news_article_doc [cnt_dis]
def get_disagreement_word_count(doc):
    cnt_dis = 0
    tokens = word_tokenize(doc)
    list_disagree = get_disagreement_words()
    for i in range(0, len(tokens), 1):
        if tokens[i] in list_disagree:
            cnt_dis = cnt_dis + 1
    return cnt_dis
def view_commenter_behavior(start_doc_index: int, end_doc_index: int):
    for i in range(start_doc_index, end_doc_index, 1):
        for j in range(0, 4, 1):
            comm_doc_name = "news_" + str(i) + "_comment_" + str(j)
            comm_text = load_doc_file("documents/comments/", comm_doc_name)
            if (comm_text != None):
                ad_word_count = []
                ad_word_count.append(('agreement', get_agreement_word_count(comm_text),))
                ad_word_count.append(('disagreement', get_disagreement_word_count(comm_text),))
                plot_histogram(pandas.Series(dict(ad_word_count)), 'Number of Agreement/Disagreement Words in ' + comm_doc_name,
                               'Word Type', 'Count')
            else:
                continue
        else:
            print('---------------------------------------------------------------------------')
            continue


## Project tasks ##
results = retrieve_search_results('carbon emissions', 40)
news_articles_w_comments = parse_news_articles(results)
get_comments_from_articles(news_articles_w_comments)
view_most_frequent_terms(1, 21)
# view_sentiments(1, 21)
# view_user_disagreement_extent(1, 21)
# view_user_agreement_extent(1, 21)
# view_commenter_behavior(1, 21)

